# Machine Learning Engineer Nanodegree
## Capstone Proposal
Juan Andrés Ramírez
September 31st, 2017

## Proposal

### Domain Background

Hand gesture recognition is an important research issue because of its extensive applications in virtual reality, sign language recognition, and computer games [1]. Sensors used for hand gesture recognition include wearable sensors such a data gloves and external sensors such as video cameras and depth cameras [2]. Data gloves usually require extensive calibration and restrict natural hand movement [2]. Video-based based approaches addresses this issues but adds other problems, like the hand segmentation from background and oclussion [2]. There are several recent works that uses depth cameras as sensors, but this hardware doesn't have the availability that video cameras have today in people's homes.

Vision-based hand gesture recognition techniques can be divided into two groups: appearance-based approaches and 3D hand model-based approaches. Appearance-based approaches use image features of the hand and compare these parameters with the extracted image features from the input. 3D hand model-based approaches rely on a 3D kinematic hand model and try to estimate the hand parameters by comparison between the input images and
possible 2D sinthetic images, generated by the 3D hand model [3].


### Problem Statement

The main objective of the proposed work is to create an image classifier capable of detecting the three different hand gestures from the rock-paper-scissors game. This classifier should allow a human user to play this game with a computer using just a screen and a webcam as an interface. To measure the classification performance it is going to be used the accuracy score over a test dataset.

### Datasets and Inputs

The proposed classifier will receive a webcam color image as input. The algorithm should work well on different light conditions, postures and backgrounds, so the training and testing datasets should reflect this noisy environments. For this purpose there's the need of using a lot of samples from different sources. The selected databases are:
* A subset of the SenseZ3D static hand gesture dataset [4, 5], which contains 30 images of different hand gestures from the same subject in webcam similar conditions. The proposed subset contains just the gestures of Rock-Paper-Scissors (G1, G2 and G5)
* A subset from the BochumGesture1998 [6], which contains 3 images per gesture from 19 different subjects. This database just considers hand pictures of size 128x128 pixels (smallest images of all considered databases).
* A specially made dataset with the webcam images of three subjects. This dataset will contain 30 images of each gesture for each subject

Database will be divided into train/test for all sources and the training subset will be *augmented* to prevent overfitting.

### Solution Statement

The proposed solution is to obtain a classifier by applying *Transfer Learning* to ResNet50 convolutional neural network. The collected rock-paper-scissors training dataset is going to be used to adapt the weights of a fully connected layer that takes its inputs from ResNet50 pre-classification outputs. The resulting algorithm is going to be evaluated as a classification task where the input are the images of people showing hand gestures from rock-paper-scissors game. The accuracy measure is going to be used to evaluate performance over the three different classes of the rock-paper-scissors game.

### Benchmark Model
_(approximately 1-2 paragraphs)_

A general hand gesture classifier may be used as a benchmark for this project. This classifiers usually work with more than three classes, but they appear to be the closest benchmarking model to this problem found in literature.

In the work done over BochumGesture1998 in [6], researchers used a very small training set (images from 3 out of 19 individuals) and a very large testing dataset. They claimed to have achieved 85.8% accuracy on images with complex background when classifiying them into 12 different hand postures. This work seems to be a little bit old, but as most of the recent work is based on range cameras, this research appears to be a good starting point for this project.

### Evaluation Metrics
_(approx. 1-2 paragraphs)_

The proposed evaluation metric is the clasiffication accuracy over the test dataset.

### Project Design
_(approx. 1 page)_

#### Data preprocessing

Example images from selected datasets:

|Dataset| Rock        | Paper           | Scissors  |
|----------| ------------- |:-------------:| -----:|
|SenseZ3D|![SenseZ3d rock](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/rock.png)|![SenseZ3d paper](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/paper.png)|![SenseZ3d scissors](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/scissors.png)
|BochumGestures1998|![Bochum rock](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/carstenk01c00R.png)|![Bochum paper](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/carstenk03c00R.png)|![Bochum scissors](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/images/rashidm11c00R.png)|
|Specially made| ![researcher rock](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/WIN_20170914_16_43_28_Pro.jpg)      | ![researcher paper](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/WIN_20170914_16_43_43_Pro.jpg) | ![researcher scissors](https://s3-us-west-2.amazonaws.com/mtcapps/mlcapstone/WIN_20170914_16_43_38_Pro.jpg) |



In this final section, summarize a theoretical workflow for approaching a solution given the problem. Provide thorough discussion for what strategies you may consider employing, what analysis of the data might be required before being used, or which algorithms will be considered for your implementation. The workflow and discussion that you provide should align with the qualities of the previous sections. Additionally, you are encouraged to include small visualizations, pseudocode, or diagrams to aid in describing the project design, but it is not required. The discussion should clearly outline your intended workflow of the capstone project.

-----------

**Before submitting your proposal, ask yourself. . .**

- Does the proposal you have written follow a well-organized structure similar to that of the project template?
- Is each section (particularly **Solution Statement** and **Project Design**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?
- Would the intended audience of your project be able to understand your proposal?
- Have you properly proofread your proposal to assure there are minimal grammatical and spelling mistakes?
- Are all the resources used for this project correctly cited and referenced?

## References
* [1] Ren, Zhou, et al. "Robust hand gesture recognition with kinect sensor." Proceedings of the 19th ACM international conference on Multimedia. ACM, 2011.
* [2] Suarez, Jesus, and Robin R. Murphy. "Hand gesture recognition with depth images: A review." Ro-man, 2012 IEEE. IEEE, 2012.
* [3] Chen, Qing, Nicolas D. Georganas, and Emil M. Petriu. "Real-time vision-based hand gesture recognition using haar-like features." Instrumentation and Measurement Technology Conference Proceedings, 2007. IMTC 2007. IEEE. IEEE, 2007.
* [4] Minto, L., and P. Zanuttigh. "Exploiting silhouette descriptors and synthetic data for hand gesture recognition." (2015).
* [5] Memo, Alvise, and Pietro Zanuttigh. "Head-mounted gesture controlled interface for human-computer interaction." Multimedia Tools and Applications (2016): 1-27.
* [6] Triesch, Jochen, and Christoph Von Der Malsburg. "A system for person-independent hand posture recognition against complex backgrounds." IEEE Transactions on Pattern Analysis and Machine Intelligence 23.12 (2001): 1449-1453.